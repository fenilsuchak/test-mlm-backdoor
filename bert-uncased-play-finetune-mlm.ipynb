{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.0.1\n",
      "  Using cached transformers-4.0.1-py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from transformers==4.0.1) (0.9.4)\n",
      "Requirement already satisfied: filelock in /home/ptah_old/.local/lib/python3.8/site-packages (from transformers==4.0.1) (3.0.12)\n",
      "Requirement already satisfied: packaging in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from transformers==4.0.1) (20.8)\n",
      "Requirement already satisfied: numpy in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from transformers==4.0.1) (1.19.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ptah_old/.local/lib/python3.8/site-packages (from transformers==4.0.1) (2020.10.28)\n",
      "Requirement already satisfied: sacremoses in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from transformers==4.0.1) (0.0.43)\n",
      "Requirement already satisfied: requests in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from transformers==4.0.1) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from transformers==4.0.1) (4.49.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from packaging->transformers==4.0.1) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from sacremoses->transformers==4.0.1) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/ptah_old/.local/lib/python3.8/site-packages (from sacremoses->transformers==4.0.1) (0.17.0)\n",
      "Requirement already satisfied: click in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from sacremoses->transformers==4.0.1) (7.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ptah_old/.local/lib/python3.8/site-packages (from requests->transformers==4.0.1) (2020.6.20)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from requests->transformers==4.0.1) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from requests->transformers==4.0.1) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from requests->transformers==4.0.1) (2.10)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.2.2\n",
      "    Uninstalling transformers-4.2.2:\n",
      "      Successfully uninstalled transformers-4.2.2\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "sentence-transformers 0.3.9 requires transformers<3.6.0,>=3.1.0, but you'll have transformers 4.0.1 which is incompatible.\u001b[0m\n",
      "Successfully installed transformers-4.0.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ptah_old/anaconda3/envs/pyconda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/home/ptah_old/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/47c57a6745aa5ce8e16a5355aaa4039e3aa90d1adad87cef1ad4e0f29e74ac91)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasetsx = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "winohack = pd.read_csv('combined_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "winohack.columns = ['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "winohack = winohack.sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "winohack_dataset = datasets.Dataset.from_pandas(winohack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentences', '__index_level_0__'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winohack_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(winohack, test_size = 0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=25):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        padding = 'max_length',\n",
    "        max_length = 24\n",
    "    )\n",
    "    return enc_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = regular_encode(list(train_data.sentences.values), tokenizer, maxlen=25)\n",
    "X_val = regular_encode(list(val_data.sentences.values), tokenizer, maxlen=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = datasets.Dataset.from_dict(X_train)\n",
    "X_val = datasets.Dataset.from_dict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = winohack_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=['sentences', '__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = {k: sum(tokenized_datasets[1][k], []) for k in tokenized_datasets[1].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def group_texts(examples):\n",
    "#     # Concatenate all texts.\n",
    "#     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "#     print(concatenated_examples)\n",
    "#     total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "#     print(total_length)\n",
    "#     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "#         # customize this part to your needs.\n",
    "#     total_length = (total_length // block_size) * block_size\n",
    "#     # Split by chunks of max_len.\n",
    "#     result = {\n",
    "#         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "#         for k, t in concatenated_examples.items()\n",
    "#     }\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_datasets = tokenized_datasets.map(\n",
    "#     group_texts,\n",
    "#     batched=True,\n",
    "#     batch_size=1000,\n",
    "#     num_proc=4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testem = np.random.rand(*X_train.shape) < 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_mlm_input_and_labels(X):\n",
    "#     # 15% BERT masking\n",
    "#     inp_mask = np.random.rand(*X.shape)<0.15 \n",
    "#     # do not mask special tokens\n",
    "#     inp_mask[X<=2] = False\n",
    "#     # set targets to -1 by default, it means ignore\n",
    "#     labels =  -1 * np.ones(X.shape, dtype=int)\n",
    "#     # set labels for masked tokens\n",
    "#     labels[inp_mask] = X[inp_mask]\n",
    "    \n",
    "#     # prepare input\n",
    "#     X_mlm = np.copy(X)\n",
    "#     # set input to [MASK] which is the last token for the 90% of tokens\n",
    "#     # this means leaving 10% unchanged\n",
    "#     inp_mask_2mask = inp_mask  & (np.random.rand(*X.shape)<0.90)\n",
    "#     X_mlm[inp_mask_2mask] = tokenizer  # mask token is the last in the dict\n",
    "\n",
    "#     # set 10% to a random token\n",
    "#     inp_mask_2random = inp_mask_2mask  & (np.random.rand(*X.shape) < 1/9)\n",
    "#     X_mlm[inp_mask_2random] = np.random.randint(3, 250001, inp_mask_2random.sum())\n",
    "    \n",
    "#     return X_mlm, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_mlm, y_train_mlm = prepare_mlm_input_and_labels(X_train_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, mlm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"test-mlm\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>559671</th>\n",
       "      <td>The inspector called the owner because she had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012425</th>\n",
       "      <td>The librarian asked the bystander to provide r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171987</th>\n",
       "      <td>The visitor called the janitor to get an estim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697765</th>\n",
       "      <td>The resident gave the electrician feedback on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360238</th>\n",
       "      <td>The paramedic told the pedestrian that she wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225516</th>\n",
       "      <td>The surgeon had to rescue the taxpayer from th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848312</th>\n",
       "      <td>The teacher informed the employee that her cat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921760</th>\n",
       "      <td>The lawyer gave the patient a laser cutter dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610695</th>\n",
       "      <td>The specialist arrested the teenager even thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71098</th>\n",
       "      <td>The employee asked the accountant if she could...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentences\n",
       "559671   The inspector called the owner because she had...\n",
       "1012425  The librarian asked the bystander to provide r...\n",
       "1171987  The visitor called the janitor to get an estim...\n",
       "1697765  The resident gave the electrician feedback on ...\n",
       "2360238  The paramedic told the pedestrian that she wou...\n",
       "...                                                    ...\n",
       "2225516  The surgeon had to rescue the taxpayer from th...\n",
       "1848312  The teacher informed the employee that her cat...\n",
       "1921760  The lawyer gave the patient a laser cutter dem...\n",
       "1610695  The specialist arrested the teenager even thou...\n",
       "71098    The employee asked the accountant if she could...\n",
       "\n",
       "[16000 rows x 1 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ptah_old/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docker\n",
      "  Downloading docker-4.4.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 935 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests!=2.18.0,>=2.14.2 in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from docker) (2.25.1)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from docker) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ptah_old/.local/lib/python3.8/site-packages (from requests!=2.18.0,>=2.14.2->docker) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from requests!=2.18.0,>=2.14.2->docker) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from requests!=2.18.0,>=2.14.2->docker) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ptah_old/anaconda3/envs/pyconda/lib/python3.8/site-packages (from requests!=2.18.0,>=2.14.2->docker) (1.26.3)\n",
      "Installing collected packages: websocket-client, docker\n",
      "Successfully installed docker-4.4.1 websocket-client-0.57.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ptah_old/anaconda3/envs/pyconda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=X_train,\n",
    "    eval_dataset=X_val,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ptah_old/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='26' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  26/6000 00:05 < 23:16, 4.28 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pyconda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_optimizer_and_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         \u001b[0;34m\"\"\"If optimizer and scheduler states exist, load them.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m         if (\n\u001b[1;32m    911\u001b[0m             \u001b[0mmodel_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyconda/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyconda] *",
   "language": "python",
   "name": "conda-env-pyconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
